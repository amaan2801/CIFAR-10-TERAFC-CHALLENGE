# -*- coding: utf-8 -*-
"""CIFAR-10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BB5vPrWBgmM9IRN2Qh3EnJPB_ywD-J8p
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision import models
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

trainset = torchvision.datasets.CIFAR10(
    root="./data",
    train=True,
    download=True,
    transform=transform
)

testset = torchvision.datasets.CIFAR10(
    root="./data",
    train=False,
    download=True,
    transform=transform
)

trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=64,
    shuffle=True
)

testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=64,
    shuffle=False
)

model = models.resnet18(pretrained=True)

num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)
model = model.to(device)

for param in model.parameters():
    param.requires_grad = False

for param in model.layer4.parameters():
    param.requires_grad = True

for param in model.fc.parameters():
    param.requires_grad = True

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=0.0005
)

epochs = 12
train_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0

    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(trainloader)
    train_losses.append(avg_loss)

    print(f"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}")

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

import os
os.makedirs("models", exist_ok=True)

torch.save(model.state_dict(), "models/level1.pth")
print("Saved Level 1 checkpoint.")

plt.plot(train_losses)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Curve - Level 1")
plt.show()

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomCrop(224, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

trainset = torchvision.datasets.CIFAR10(
    root="./data",
    train=True,
    download=True,
    transform=train_transform
)

testset = torchvision.datasets.CIFAR10(
    root="./data",
    train=False,
    download=True,
    transform=test_transform
)

trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=64, shuffle=True
)

testloader = torch.utils.data.DataLoader(
    testset, batch_size=64, shuffle=False
)

model = models.resnet18(pretrained=True)

num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)
model = model.to(device)

for param in model.parameters():
    param.requires_grad = False

for param in model.layer4.parameters():
    param.requires_grad = True

for param in model.fc.parameters():
    param.requires_grad = True

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=0.0005,
    weight_decay=1e-4
)

epochs = 12
train_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0

    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(trainloader)
    train_losses.append(avg_loss)

    print(f"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}")

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Level 2 Test Accuracy: {accuracy:.2f}%")

import os
os.makedirs("models", exist_ok=True)

torch.save(model.state_dict(), "models/level2.pth")
print("Saved Level 2 checkpoint.")

# Level 2 : No Augmentation (Baseline Transform)

no_aug_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

no_aug_testset = torchvision.datasets.CIFAR10(
    root="./data",
    train=False,
    download=True,
    transform=no_aug_transform
)

no_aug_testloader = torch.utils.data.DataLoader(
    no_aug_testset, batch_size=64, shuffle=False
)

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in no_aug_testloader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

no_aug_accuracy = 100 * correct / total
print(f"Level 2 Accuracy WITHOUT augmentation: {no_aug_accuracy:.2f}%")

print(f"Level 2 Accuracy WITH augmentation: {accuracy:.2f}%")
print("\nLEVEL 2 ABLATION STUDY")
print("-" * 35)
print(f"Without Augmentation : {no_aug_accuracy:.2f}%")
print(f"With Augmentation    : {accuracy:.2f}%")
print(f"Improvement          : {accuracy - no_aug_accuracy:.2f}%")

## level3

class Level3ResNet(nn.Module):
    def __init__(self, num_classes=10):
        super(Level3ResNet, self).__init__()

        base = models.resnet18(pretrained=True)


        self.features = nn.Sequential(*list(base.children())[:-1])


        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

model3 = Level3ResNet(num_classes=10).to(device)

for param in model3.features.parameters():
    param.requires_grad = False


for param in model3.features[-2].parameters():
    param.requires_grad = True

for param in model3.features[-1].parameters():
    param.requires_grad = True


for param in model3.classifier.parameters():
    param.requires_grad = True

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(
    [
        {"params": model3.features[-2].parameters(), "lr": 1e-4},
        {"params": model3.features[-1].parameters(), "lr": 1e-4},
        {"params": model3.classifier.parameters(), "lr": 5e-4},
    ],
    weight_decay=1e-4
)

epochs = 8
train_losses = []

for epoch in range(epochs):
    model3.train()
    running_loss = 0.0

    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model3(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(trainloader)
    train_losses.append(avg_loss)

    print(f"Level 3 | Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}")

model3.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        outputs = model3(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Level 3 Test Accuracy: {accuracy:.2f}%")

import os
os.makedirs("models", exist_ok=True)

torch.save(model.state_dict(), "models/level3.pth")
print("Saved Level 3 checkpoint.")

# Initialize Counters
classes = trainset.classes   # CIFAR-10 class names

class_correct = [0 for _ in range(10)]
class_total = [0 for _ in range(10)]

# Compute Per-Class Accuracy
model3.eval()

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)

        outputs = model3(images)
        _, predicted = torch.max(outputs, 1)

        for i in range(labels.size(0)):
            label = labels[i]
            class_correct[label] += (predicted[i] == label).item()
            class_total[label] += 1

print("\nPer-class accuracy (Level 3):")
for i in range(10):
    acc = 100 * class_correct[i] / class_total[i]
    print(f"{classes[i]:>10s}: {acc:.2f}%")

# VISUALIZATION / INTERPRETABILITY â€” GRAD-CAM
import torch.nn.functional as F

gradients = None
activations = None

def forward_hook(module, input, output):
    global activations
    activations = output

def backward_hook(module, grad_input, grad_output):
    global gradients
    gradients = grad_output[0]

# CORRECT last convolution layer for ResNet18
target_layer = model3.features[-2][-1].conv2

target_layer.register_forward_hook(forward_hook)
target_layer.register_backward_hook(backward_hook)

def generate_gradcam(model, image, class_index):
    model.eval()

    image = image.unsqueeze(0).to(device)

    output = model(image)
    model.zero_grad()

    loss = output[0, class_index]
    loss.backward()

    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])
    activation = activations.squeeze(0)

    for i in range(activation.shape[0]):
        activation[i] *= pooled_gradients[i]

    heatmap = torch.mean(activation, dim=0)
    heatmap = F.relu(heatmap)
    heatmap /= torch.max(heatmap)

    return heatmap.detach().cpu()

# Take one test image
image, label = testset[0]

# Generate Grad-CAM
heatmap = generate_gradcam(model3, image, label)

# Plot
plt.figure(figsize=(8, 4))

plt.subplot(1, 2, 1)
plt.imshow(image.permute(1, 2, 0))
plt.title(f"Original Image (Class: {classes[label]})")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(heatmap, cmap="jet")
plt.title("Grad-CAM Heatmap")
plt.axis("off")

plt.show()



#############################

# level 4
model   # Level 2 model
model3  # Level 3 model

model.eval()    # Level 2 model
model3.eval()   # Level 3 model

def ensemble_predict(model_a, model_b, images):
    with torch.no_grad():
        out_a = model_a(images)
        out_b = model_b(images)

        # Average logits
        ensemble_output = (out_a + out_b) / 2
        _, predicted = torch.max(ensemble_output, 1)

    return predicted

correct = 0
total = 0

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)

        preds = ensemble_predict(model, model3, images)

        total += labels.size(0)
        correct += (preds == labels).sum().item()

ensemble_accuracy = 100 * correct / total
print(f"Level 4 Ensemble Accuracy: {ensemble_accuracy:.2f}%")

classes = trainset.classes

class_correct = [0] * 10
class_total = [0] * 10

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        preds = ensemble_predict(model, model3, images)

        for i in range(labels.size(0)):
            label = labels[i]
            class_correct[label] += (preds[i] == label).item()
            class_total[label] += 1

print("\nLevel 4 Per-class Accuracy (Ensemble):")
for i in range(10):
    acc = 100 * class_correct[i] / class_total[i]
    print(f"{classes[i]:>10s}: {acc:.2f}%")

import os
os.makedirs("models", exist_ok=True)

torch.save({
    "level2_model": model.state_dict(),
    "level3_model": model3.state_dict()
}, "models/level4_ensemble.pth")

print("Saved Level 4 ensemble checkpoint.")

print("\nLEVEL 4 COMPARATIVE ANALYSIS")
print("-" * 40)
print(f"Level 2 Accuracy : {accuracy:.2f}%")
print(f"Level 3 Accuracy : {accuracy:.2f}%  (custom model)")
print(f"Level 4 Accuracy : {ensemble_accuracy:.2f}%  (ensemble)")





